{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be77d8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 22:59:15,555\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-10 22:59:29,491\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-10 22:59:29,976\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray.rllib.algorithms.a2c import A2CConfig\n",
    "import numpy as np\n",
    "\n",
    "import dm_memorytasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 22:59:40,607\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-10 22:59:40,609\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-10-10 22:59:44,029\tINFO worker.py:1642 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "# Define your problem using python and Farama-Foundation's gymnasium API:\n",
    "class ParrotEnv(gym.Env):\n",
    "    \"\"\"Environment in which an agent must learn to repeat the seen observations.\n",
    "\n",
    "    Observations are float numbers indicating the to-be-repeated values,\n",
    "    e.g. -1.0, 5.1, or 3.2.\n",
    "\n",
    "    The action space is always the same as the observation space.\n",
    "\n",
    "    Rewards are r=-abs(observation - action), for all steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # Make the space (for actions and observations) configurable.\n",
    "        self.action_space = config.get(\n",
    "            \"parrot_shriek_range\", gym.spaces.Box(-1.0, 1.0, shape=(1, )))\n",
    "        # Since actions should repeat observations, their spaces must be the\n",
    "        # same.\n",
    "        self.observation_space = self.action_space\n",
    "        self.cur_obs = None\n",
    "        self.episode_len = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\n",
    "        \"\"\"\n",
    "        # Reset the episode len.\n",
    "        self.episode_len = 0\n",
    "        # Sample a random number from our observation space.\n",
    "        self.cur_obs = self.observation_space.sample()\n",
    "        # Return initial observation.\n",
    "        return self.cur_obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given `action`\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, done-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "        # Set `truncated` flag after 10 steps.\n",
    "        self.episode_len += 1\n",
    "        terminated = False\n",
    "        truncated = self.episode_len >= 10\n",
    "        # r = -abs(obs - action)\n",
    "        reward = -sum(abs(self.cur_obs - action))\n",
    "        # Set a new observation (random sample).\n",
    "        self.cur_obs = self.observation_space.sample()\n",
    "        return self.cur_obs, reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "# Create an RLlib Algorithm instance from a PPOConfig to learn how to\n",
    "# act in the above environment.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        env=ParrotEnv,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        env_config={\n",
    "            \"parrot_shriek_range\": gym.spaces.Box(-5.0, 5.0, (1, ))\n",
    "        },\n",
    "    )\n",
    "    # Parallelize environment rollouts.\n",
    "    .rollouts(num_rollout_workers=3)\n",
    ")\n",
    "# Use the config's `build()` method to construct a PPO object.\n",
    "algo = config.build()\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to guess 10 times and the optimal reward is 0.0\n",
    "# (exact match between observation and action value),\n",
    "# we can expect to reach an optimal episode reward of 0.0.\n",
    "for i in range(5):\n",
    "    results = algo.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e08efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for psych lab -> gymnasium env\n",
    "class PsychLab(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "\n",
    "        # Initialize the PsychLab environment with the provided config\n",
    "        env_settings = dm_memorytasks.EnvironmentSettings(seed=123, level_name='spot_diff_extrapolate')\n",
    "        \n",
    "        self.env = dm_memorytasks.load_from_docker(name='gcr.io/deepmind-environments/dm_memorytasks:v1.0.1', settings=env_settings)        \n",
    "        self.action_spec = self.env.action_spec()\n",
    "        observation_spec = self.env.observation_spec()\n",
    "\n",
    "        self.action_space = gym.spaces.Dict({\n",
    "            'STRAFE_LEFT_RIGHT': gym.spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32),\n",
    "            'MOVE_BACK_FORWARD': gym.spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32),\n",
    "            'LOOK_LEFT_RIGHT': gym.spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32),\n",
    "            'LOOK_DOWN_UP': gym.spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=observation_spec['RGB_INTERLEAVED'].shape, dtype=int)\n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Reset the PsychLab environment and return the initial observation\n",
    "        timestep = self.env.reset()\n",
    "        return timestep.observation['RGB_INTERLEAVED'], {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        timestep = self.env.step(action)\n",
    "        print('goblin')\n",
    "        print(timestep)\n",
    "        return timestep.observation['RGB_INTERLEAVED'], timestep.reward, False, timestep.last(),{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595074ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 22:50:34,515\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a2c/` has been deprecated. Use `rllib_contrib/a2c/` instead. This will raise an error in the future!\n",
      "2023-10-10 22:50:34,517\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/common/home/ac1771/.conda/envs/recall_agent/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-10-10 22:50:37,805\tINFO worker.py:1642 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Create an RLlib Algorithm instance\n",
    "config = A2CConfig()\n",
    "# config = config.training(lr=0.0003, train_batch_size=52)  \n",
    "config = config.resources(num_gpus=0)\n",
    "\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=PsychLab)\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "for i in range(5):\n",
    "    results = algo.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
